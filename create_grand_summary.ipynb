{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 13:52:50.179872: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-14 13:52:50.184780: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-14 13:52:50.242401: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 13:52:51.382477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import auc\n",
    "from os.path import join\n",
    "from evaluator import Evaluator, CropOffsetFilter\n",
    "from training_models import Autoencoder, DenoisingAutoencoder, AutoencoderEnsemble\n",
    "from monitor_models import Autocovariance, AugmentationCrossCovariances, RepresentationCrossCovariances\n",
    "from metrics import Identity, Variance, CovarianceWeightedAverage\n",
    "from directory import get_history_csv_path, initiate_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"exp_test\"\n",
    "TRAIN_DATASETS = [\"stead\"]\n",
    "TEST_DATASETS = [\"stead\"]\n",
    "SPLITS = range(0, 5)\n",
    "NUM_EPOCHS = 20\n",
    "FILTERS = [CropOffsetFilter(3.0, 3.0, 30.0, 100.0)]\n",
    "OUTPUT_FILE_NAME = \"summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiate_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roc_aucs(training_model_class, monitoring_model_class, train_dataset, test_dataset, split, method_params, metric):\n",
    "    evaluator = Evaluator(EXP_NAME, training_model_class, monitoring_model_class, \n",
    "                          train_dataset, test_dataset, FILTERS, range(NUM_EPOCHS), split, method_params, metric)\n",
    "                    \n",
    "    roc_vectors = evaluator.get_roc_vectors()\n",
    "    roc_aucs = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        roc_aucs.append(auc(roc_vectors[epoch][\"tpr\"], roc_vectors[epoch][\"fpr\"]))\n",
    "\n",
    "    return roc_aucs\n",
    "\n",
    "def get_validation_losses(training_model_class, train_dataset, split):\n",
    "    model = training_model_class()\n",
    "    df_history = pd.read_csv(get_history_csv_path(EXP_NAME, model.name, train_dataset, split))\n",
    "    return df_history.loc[:, \"val_loss\"]\n",
    "\n",
    "def model_summary(training_model_class, monitoring_model_class, method_params, metric):\n",
    "    evaluations = []\n",
    "    for train_dataset in TRAIN_DATASETS:\n",
    "        for test_dataset in TEST_DATASETS:\n",
    "            for split in SPLITS:\n",
    "                roc_aucs = calculate_roc_aucs(training_model_class, monitoring_model_class, train_dataset, test_dataset, split, method_params, metric)\n",
    "                val_loss = get_validation_losses(training_model_class, train_dataset, split)\n",
    "                evaluations.append({\"Training dataset\": [train_dataset]*NUM_EPOCHS, \n",
    "                                    \"Testing dataset\": [test_dataset]*NUM_EPOCHS, \n",
    "                                    \"Split\": [split]*NUM_EPOCHS, \n",
    "                                    \"Epoch\": range(NUM_EPOCHS), \n",
    "                                    \"Validation loss\": val_loss, \n",
    "                                    \"ROC-AUC\":roc_aucs})\n",
    "\n",
    "    df = pd.DataFrame(evaluations)\n",
    "\n",
    "    df.loc[:, \"Training model\"] = training_model_class().name\n",
    "    df.loc[:, \"Monitoring model\"] = monitoring_model_class().name\n",
    "\n",
    "    return df\n",
    "\n",
    "def grand_summary():\n",
    "    df_autoencoder_autocovariance = model_summary(Autoencoder, Autocovariance, \n",
    "                                                  {}, \n",
    "                                                  CovarianceWeightedAverage(input_param=\"fcov\"))\n",
    "    \n",
    "    df_denoisingae_autocovariance = model_summary(DenoisingAutoencoder, Autocovariance, \n",
    "                                                  {}, \n",
    "                                                  CovarianceWeightedAverage(input_param=\"fcov\"))\n",
    "    \n",
    "    df_autoencoder_augmentation_crosscov = model_summary(Autoencoder, AugmentationCrossCovariances, \n",
    "                                                         {\"augmentations\": 5, \"std\": 0.15, \"knots\": 4}, \n",
    "                                                         CovarianceWeightedAverage(input_param=\"fcov\"))\n",
    "    \n",
    "    df_denoisingae_augmentation_crosscov = model_summary(DenoisingAutoencoder, AugmentationCrossCovariances, \n",
    "                                                         {\"augmentations\": 5, \"std\": 0.15, \"knots\": 4}, \n",
    "                                                         CovarianceWeightedAverage(input_param=\"fcov\"))\n",
    "    \n",
    "    df_autoencoder_ensemble_representation_crosscov = model_summary(AutoencoderEnsemble, RepresentationCrossCovariances, \n",
    "                                                                    {}, \n",
    "                                                                    CovarianceWeightedAverage(input_param=\"fcov\"))\n",
    "\n",
    "    df_grand_summary = pd.concat(\n",
    "        [\n",
    "            df_autoencoder_autocovariance,\n",
    "            df_denoisingae_autocovariance,\n",
    "            df_autoencoder_augmentation_crosscov,\n",
    "            df_denoisingae_augmentation_crosscov,\n",
    "            df_autoencoder_ensemble_representation_crosscov,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df_grand_summary.sort_values(\n",
    "        by=[\n",
    "            \"Monitoring model\",\n",
    "            \"Training model\",\n",
    "            \"Training dataset\",\n",
    "            \"Testing dataset\",\n",
    "            \"Epoch\",\n",
    "            \"Split\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return df_grand_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling Autoencoder.call().\n\n\u001b[1mtoo many positional arguments\u001b[0m\n\nArguments received by Autoencoder.call():\n  • inputs=tf.Tensor(shape=(256, 3000, 3), dtype=float32)\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m grand_summary()\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(join(EXP_NAME, OUTPUT_FILE_NAME))\n",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m, in \u001b[0;36mgrand_summary\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrand_summary\u001b[39m():\n\u001b[0;32m---> 40\u001b[0m     df_autoencoder_autocovariance \u001b[38;5;241m=\u001b[39m model_summary(Autoencoder, Autocovariance, \n\u001b[1;32m     41\u001b[0m                                                   {}, \n\u001b[1;32m     42\u001b[0m                                                   CovarianceWeightedAverage(input_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfcov\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     44\u001b[0m     df_denoisingae_autocovariance \u001b[38;5;241m=\u001b[39m model_summary(DenoisingAutoencoder, Autocovariance, \n\u001b[1;32m     45\u001b[0m                                                   {}, \n\u001b[1;32m     46\u001b[0m                                                   CovarianceWeightedAverage(input_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfcov\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     48\u001b[0m     df_autoencoder_augmentation_crosscov \u001b[38;5;241m=\u001b[39m model_summary(Autoencoder, AugmentationCrossCovariances, \n\u001b[1;32m     49\u001b[0m                                                          {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maugmentations\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknots\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m}, \n\u001b[1;32m     50\u001b[0m                                                          CovarianceWeightedAverage(input_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfcov\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36mmodel_summary\u001b[0;34m(training_model_class, monitoring_model_class, method_params, metric)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_dataset \u001b[38;5;129;01min\u001b[39;00m TEST_DATASETS:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m SPLITS:\n\u001b[0;32m---> 23\u001b[0m         roc_aucs \u001b[38;5;241m=\u001b[39m calculate_roc_aucs(training_model_class, monitoring_model_class, train_dataset, test_dataset, split, method_params, metric)\n\u001b[1;32m     24\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m get_validation_losses(training_model_class, train_dataset, split)\n\u001b[1;32m     25\u001b[0m         evaluations\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: [train_dataset]\u001b[38;5;241m*\u001b[39mNUM_EPOCHS, \n\u001b[1;32m     26\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: [test_dataset]\u001b[38;5;241m*\u001b[39mNUM_EPOCHS, \n\u001b[1;32m     27\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplit\u001b[39m\u001b[38;5;124m\"\u001b[39m: [split]\u001b[38;5;241m*\u001b[39mNUM_EPOCHS, \n\u001b[1;32m     28\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS), \n\u001b[1;32m     29\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_loss, \n\u001b[1;32m     30\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC-AUC\u001b[39m\u001b[38;5;124m\"\u001b[39m:roc_aucs})\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mcalculate_roc_aucs\u001b[0;34m(training_model_class, monitoring_model_class, train_dataset, test_dataset, split, method_params, metric)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_roc_aucs\u001b[39m(training_model_class, monitoring_model_class, train_dataset, test_dataset, split, method_params, metric):\n\u001b[1;32m      2\u001b[0m     evaluator \u001b[38;5;241m=\u001b[39m Evaluator(EXP_NAME, training_model_class, monitoring_model_class, \n\u001b[1;32m      3\u001b[0m                           train_dataset, test_dataset, FILTERS, \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS), split, method_params, metric)\n\u001b[0;32m----> 5\u001b[0m     roc_vectors \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mget_roc_vectors()\n\u001b[1;32m      6\u001b[0m     roc_aucs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n",
      "File \u001b[0;32m~/Documents/EARTH-ML/LatentCovarianceBasedSeismicEventDetection/main/evaluator.py:162\u001b[0m, in \u001b[0;36mEvaluator.get_roc_vectors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_roc_vectors\u001b[39m(\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    160\u001b[0m ):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_are_monitoring_files_present():\n\u001b[0;32m--> 162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtester\u001b[38;5;241m.\u001b[39mtest()\n\u001b[1;32m    164\u001b[0m     monitoring_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_monitoring_meta()\n\u001b[1;32m    165\u001b[0m     monitoring_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_filters(monitoring_meta)\n",
      "File \u001b[0;32m~/Documents/EARTH-ML/LatentCovarianceBasedSeismicEventDetection/main/kfold_tester.py:46\u001b[0m, in \u001b[0;36mKFoldTester.test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m __, __, __, predict_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_environment\u001b[38;5;241m.\u001b[39mget_generators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs:\n\u001b[0;32m---> 46\u001b[0m     monitoring_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_monitoring_model(epoch)\n\u001b[1;32m     47\u001b[0m     monitoring_dict \u001b[38;5;241m=\u001b[39m monitoring_model\u001b[38;5;241m.\u001b[39mpredict(predict_gen)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_data_file(monitoring_dict, epoch)\n",
      "File \u001b[0;32m~/Documents/EARTH-ML/LatentCovarianceBasedSeismicEventDetection/main/kfold_tester.py:95\u001b[0m, in \u001b[0;36mKFoldTester._create_monitoring_model\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_monitoring_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_ctor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m         training_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_training_model()\n\u001b[1;32m     97\u001b[0m         training_model\u001b[38;5;241m.\u001b[39mload_weights(\n\u001b[1;32m     98\u001b[0m             get_checkpoint_path(\n\u001b[1;32m     99\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexp_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m             )\n\u001b[1;32m    105\u001b[0m         )\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/EARTH-ML/LatentCovarianceBasedSeismicEventDetection/main/kfold_tester.py:89\u001b[0m, in \u001b[0;36mKFoldTester._create_training_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_ctor()\n\u001b[1;32m     84\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     85\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[1;32m     86\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     87\u001b[0m )\n\u001b[0;32m---> 89\u001b[0m model(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(shape\u001b[38;5;241m=\u001b[39m(BATCH_SIZE, model\u001b[38;5;241m.\u001b[39mN_TIMESTEPS, model\u001b[38;5;241m.\u001b[39mN_CHANNELS)))\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/dsml/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/EARTH-ML/LatentCovarianceBasedSeismicEventDetection/main/training_models.py:36\u001b[0m, in \u001b[0;36mAutoencoder.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp(inputs)\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(x, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/dsml/lib/python3.12/inspect.py:3259\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bind(args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dsml/lib/python3.12/inspect.py:3180\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3178\u001b[0m     param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(parameters)\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m-> 3180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo many positional arguments\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[1;32m   3183\u001b[0m         \u001b[38;5;66;03m# Looks like we have no parameter for this positional\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m         \u001b[38;5;66;03m# argument\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling Autoencoder.call().\n\n\u001b[1mtoo many positional arguments\u001b[0m\n\nArguments received by Autoencoder.call():\n  • inputs=tf.Tensor(shape=(256, 3000, 3), dtype=float32)\n  • training=False"
     ]
    }
   ],
   "source": [
    "df = grand_summary()\n",
    "df.to_csv(join(EXP_NAME, OUTPUT_FILE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
